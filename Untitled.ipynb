{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from FL.agents import *\n",
    "from FL.models import *\n",
    "from FL.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 5\n",
    "n_epochs = 1000\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "noniid = False\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Import Datasets\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='../data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Batch Loaders\n",
    "if noniid:\n",
    "    def noniid_batch_trainset(trainset, c):\n",
    "        indices = (np.array(trainset.targets) == c)\n",
    "        trainset2 = copy.deepcopy(trainset)\n",
    "        trainset2.data = trainset2.data[indices]\n",
    "        trainset2.targets = [c for i in range(len(indices))]\n",
    "\n",
    "        return trainset2\n",
    "\n",
    "    trainsets = [noniid_batch_trainset(trainset,i) for i in set(trainset.targets)]\n",
    "else:\n",
    "    trainsets = [trainset]\n",
    "\n",
    "samplers = [torch.utils.data.RandomSampler(i, replacement=True) for i in trainsets]\n",
    "trainloaders = [torch.utils.data.DataLoader(\n",
    "    trainsets[i], batch_size=batch_size, shuffle=False, sampler=samplers[i],\n",
    "    num_workers=0) for i in range(len(trainsets))]\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregator rule: mean\n",
    "def rule(ups_list):  # ups_list is a list of list of tensors\n",
    "    return [torch.stack([x[i] for x in ups_list]).mean(0)\n",
    "            for i in range(len(ups_list[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup Learning Model\n",
    "model = PerformantNet1()\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(\"PerformantNet1_10epochs.pt\"))\n",
    "    n_epochs = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Federated Learning Framework\n",
    "central = Central(model, optimizer)\n",
    "worker_list = []\n",
    "for i in range(n_workers):\n",
    "    worker_list.append(Worker(loss))\n",
    "agg = Agg(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.305928\n",
      "2.2995133\n",
      "2.3038704\n",
      "2.3002784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/1000 [00:11<3:13:43, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3035467\n",
      "Avg. Loss: 2.3026273250579834\n",
      "2.305126\n",
      "2.3083975\n",
      "2.3031292\n",
      "2.3014648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/1000 [00:23<3:13:46, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3026724\n",
      "Avg. Loss: 2.3041579723358154\n",
      "2.304352\n",
      "2.2978926\n",
      "2.2973106\n",
      "2.3069046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1000 [00:34<3:13:13, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.309531\n",
      "Avg. Loss: 2.3031983375549316\n",
      "2.3027136\n",
      "2.3110538\n",
      "2.3057003\n",
      "2.3036351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1000 [00:46<3:12:12, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3003535\n",
      "Avg. Loss: 2.3046913146972656\n",
      "2.2980118\n",
      "2.3001523\n",
      "2.3028452\n",
      "2.3062322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/1000 [00:57<3:12:00, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3085353\n",
      "Avg. Loss: 2.3031554222106934\n",
      "2.298717\n",
      "2.2983627\n",
      "2.303865\n",
      "2.288769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 6/1000 [01:09<3:10:49, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2911859\n",
      "Avg. Loss: 2.296180009841919\n",
      "2.3000922\n",
      "2.284395\n",
      "2.2924833\n",
      "2.2583342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 7/1000 [01:20<3:10:21, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2626858\n",
      "Avg. Loss: 2.2795979976654053\n",
      "2.2947316\n",
      "2.2157772\n",
      "2.2785218\n",
      "2.2123134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 8/1000 [01:32<3:09:57, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2912982\n",
      "Avg. Loss: 2.258528232574463\n",
      "2.2373345\n",
      "2.2078683\n",
      "2.243802\n",
      "2.2581875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 9/1000 [01:43<3:08:57, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2575805\n",
      "Avg. Loss: 2.240954637527466\n",
      "2.2598557\n",
      "2.2529771\n",
      "2.2911768\n",
      "2.2385538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 10/1000 [01:55<3:08:59, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.272146\n",
      "Avg. Loss: 2.262941837310791\n",
      "2.2147777\n",
      "2.2157502\n",
      "2.2125566\n",
      "2.213493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 11/1000 [02:06<3:08:47, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2261937\n",
      "Avg. Loss: 2.2165541648864746\n",
      "2.2648177\n",
      "2.1370757\n",
      "2.147506\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-69b7d0d1e350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mworker_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd_bkwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mweight_ups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Lev's Files/CS143/project/FL/agents.py\u001b[0m in \u001b[0;36mfwd_bkwd\u001b[0;34m(self, inp, outp)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mlossval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mweightgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = []\n",
    "accuracies = []\n",
    "\n",
    "# Training Loop\n",
    "for t in tqdm(range(n_epochs)):\n",
    "    first_time = time.time()\n",
    "\n",
    "    weight_ups = []\n",
    "    losses = []\n",
    "    central.model.train()\n",
    "\n",
    "    dataiters = [iter(trainloader) for trainloader in trainloaders]\n",
    "\n",
    "    # Worker Loop\n",
    "    for i in range(n_workers):\n",
    "        k = np.random.randint(0, len(dataiters))\n",
    "        dataiter = dataiters[k]\n",
    "\n",
    "        batch_inp, batch_outp = dataiter.next()\n",
    "        batch_inp, batch_outp = batch_inp.to(device), batch_outp.to(device)\n",
    "\n",
    "        worker_list[i].model = central.model\n",
    "\n",
    "        ups, loss = worker_list[i].fwd_bkwd(batch_inp, batch_outp)\n",
    "        print(loss)\n",
    "        weight_ups.append(ups)\n",
    "        losses.append(loss)\n",
    "\n",
    "    # Aggregate Worker Gradients\n",
    "    weight_ups_FIN = agg.rule(weight_ups)\n",
    "    \n",
    "    print('Avg. Loss: {}'.format(np.mean(losses)))\n",
    "\n",
    "    # Update Central Model\n",
    "    central.update_model(weight_ups_FIN)\n",
    "\n",
    "    central.model.eval()\n",
    "\n",
    "    if t > 0 and t % 100 == 0:\n",
    "        print('Epoch: {}, Time to complete: {}'.format(t, time.time() - first_time))\n",
    "\n",
    "    if t % 250 == 0 and t > 0:\n",
    "        # print('Epoch: {}'.format(t))\n",
    "        accuracy = print_test_accuracy(model, testloader)\n",
    "        epochs.append(t)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "print('Done training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
