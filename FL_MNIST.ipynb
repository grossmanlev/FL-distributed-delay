{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning on MNIST using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and model specifications\n",
    "\n",
    "First we make the official imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And than those specific to PySyft. In particular we define remote workers `alice` and `bob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the setting of the learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and sending to workers\n",
    "We first load the data and transform the training Dataset into a Federated Dataset split across the workers using the `.federate` method. This federated dataset is now given to a Federated DataLoader. The test dataset remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:00, 16648958.51it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 444041.88it/s]\n",
      "  1%|          | 16384/1648877 [00:00<00:11, 146612.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 6450865.81it/s]                           \n",
      "8192it [00:00, 181632.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN specification\n",
    "Here we use exactly the same CNN as in the official example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the train and test functions\n",
    "For the train function, because the data batches are distributed across `alice` and `bob`, you need to send the model to the right location for each batch. Then, you perform all the operations remotely with the same syntax like you're doing local PyTorch. When you're done, you get back the model updated and the loss to look for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function does not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305134\n",
      "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.156802\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.896600\n",
      "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.440339\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.866960\n",
      "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.654266\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.593402\n",
      "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.455609\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.370832\n",
      "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.303737\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.314273\n",
      "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.369014\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.238090\n",
      "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.187788\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.523821\n",
      "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.225225\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.143864\n",
      "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.268484\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.186659\n",
      "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.305151\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.239794\n",
      "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.257000\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.192707\n",
      "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.174481\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.221222\n",
      "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.322599\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.275114\n",
      "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.129852\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.183559\n",
      "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.222766\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.081341\n",
      "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.143047\n",
      "\n",
      "Test set: Average loss: 0.1578, Accuracy: 9508/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.103048\n",
      "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.105996\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.147700\n",
      "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.148812\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.108734\n",
      "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.111015\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.117977\n",
      "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.062927\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.089150\n",
      "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.156886\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.160407\n",
      "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.156542\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.231673\n",
      "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.198755\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.206216\n",
      "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.080726\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.063547\n",
      "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.158626\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.157262\n",
      "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.074856\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.161806\n",
      "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.073418\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.152575\n",
      "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.047740\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.086060\n",
      "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.100778\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.155095\n",
      "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.032042\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.072814\n",
      "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.114057\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.113229\n",
      "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.068069\n",
      "\n",
      "Test set: Average loss: 0.0901, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.080588\n",
      "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.079625\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.184044\n",
      "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.072051\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.078653\n",
      "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.206098\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.248576\n",
      "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.122501\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.179076\n",
      "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.150400\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.029123\n",
      "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.046800\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.054928\n",
      "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.034698\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.290224\n",
      "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.055096\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.064034\n",
      "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.025564\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.070745\n",
      "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.038754\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.033499\n",
      "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.082014\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.169878\n",
      "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.053489\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.073374\n",
      "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.024156\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.097889\n",
      "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.026865\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.055903\n",
      "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.068545\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.057902\n",
      "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.464730\n",
      "\n",
      "Test set: Average loss: 0.0740, Accuracy: 9758/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.146848\n",
      "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.064075\n",
      "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.045472\n",
      "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.100454\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.038562\n",
      "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.068525\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.118272\n",
      "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.106176\n",
      "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.046723\n",
      "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.048345\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.073632\n",
      "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.084357\n",
      "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.025807\n",
      "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.143911\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.176947\n",
      "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.094681\n",
      "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.018696\n",
      "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.148325\n",
      "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.029687\n",
      "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.010120\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.044572\n",
      "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.049077\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.136316\n",
      "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.058220\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.020831\n",
      "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.039450\n",
      "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.164171\n",
      "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.067703\n",
      "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.064767\n",
      "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.073357\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.062561\n",
      "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.017102\n",
      "\n",
      "Test set: Average loss: 0.0549, Accuracy: 9810/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.082472\n",
      "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.038683\n",
      "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.029238\n",
      "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.035155\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.063652\n",
      "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.022083\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.165204\n",
      "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.046510\n",
      "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.102623\n",
      "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.055133\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.034007\n",
      "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.100107\n",
      "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.031469\n",
      "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.026444\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.022207\n",
      "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.018012\n",
      "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.038853\n",
      "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.071408\n",
      "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.112857\n",
      "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.038043\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.029231\n",
      "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.011942\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.025715\n",
      "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.017744\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.022657\n",
      "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.015447\n",
      "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.065223\n",
      "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.038669\n",
      "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.016745\n",
      "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.096682\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.027059\n",
      "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.062130\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.052670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.103697\n",
      "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.011604\n",
      "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.090397\n",
      "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.112624\n",
      "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.014472\n",
      "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.087826\n",
      "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.037394\n",
      "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.098747\n",
      "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.052957\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.043855\n",
      "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.073001\n",
      "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.047514\n",
      "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.007711\n",
      "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.019910\n",
      "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.034876\n",
      "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.084788\n",
      "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.010765\n",
      "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.026387\n",
      "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.048259\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.006613\n",
      "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.076208\n",
      "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.041295\n",
      "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.007546\n",
      "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.065694\n",
      "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.052443\n",
      "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.026757\n",
      "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.031499\n",
      "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.054162\n",
      "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.024408\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.099303\n",
      "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.012223\n",
      "\n",
      "Test set: Average loss: 0.0443, Accuracy: 9858/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.049274\n",
      "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.021663\n",
      "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.013319\n",
      "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.040848\n",
      "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.108272\n",
      "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.026562\n",
      "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.077065\n",
      "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.018775\n",
      "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.027147\n",
      "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.163918\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.093377\n",
      "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.063692\n",
      "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.015308\n",
      "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.084813\n",
      "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.020230\n",
      "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.012855\n",
      "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.009537\n",
      "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.059701\n",
      "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.024334\n",
      "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.028904\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.043430\n",
      "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.036308\n",
      "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.012460\n",
      "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.009310\n",
      "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.059644\n",
      "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.044069\n",
      "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.068332\n",
      "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.036336\n",
      "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.036883\n",
      "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.021398\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.173389\n",
      "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.026681\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.056233\n",
      "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.036744\n",
      "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.075976\n",
      "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.077840\n",
      "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.017280\n",
      "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.010823\n",
      "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.031518\n",
      "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.132658\n",
      "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.007990\n",
      "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.032062\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.024599\n",
      "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.002997\n",
      "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.092584\n",
      "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.047526\n",
      "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.002834\n",
      "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.002491\n",
      "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.042284\n",
      "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.058117\n",
      "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.016339\n",
      "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.030521\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.070668\n",
      "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.048767\n",
      "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.046176\n",
      "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.019762\n",
      "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.055269\n",
      "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.028561\n",
      "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.014245\n",
      "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.042051\n",
      "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.036661\n",
      "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.009650\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.084662\n",
      "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.136170\n",
      "\n",
      "Test set: Average loss: 0.0364, Accuracy: 9885/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.005193\n",
      "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.011835\n",
      "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.019625\n",
      "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.009493\n",
      "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.062367\n",
      "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.018327\n",
      "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.070280\n",
      "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.006728\n",
      "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.021946\n",
      "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.040300\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.023313\n",
      "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.007181\n",
      "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.070329\n",
      "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.031150\n",
      "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.033929\n",
      "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.124040\n",
      "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.043007\n",
      "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.050868\n",
      "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.027272\n",
      "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.034663\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.010767\n",
      "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.019548\n",
      "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.033135\n",
      "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.004417\n",
      "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.049949\n",
      "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.014046\n",
      "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.006420\n",
      "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.072951\n",
      "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.019051\n",
      "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.006692\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.011017\n",
      "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.006035\n",
      "\n",
      "Test set: Average loss: 0.0345, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.004290\n",
      "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.031423\n",
      "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.029840\n",
      "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.002054\n",
      "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.087370\n",
      "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.032033\n",
      "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.037314\n",
      "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.009595\n",
      "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.002539\n",
      "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.036681\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.144622\n",
      "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.034363\n",
      "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.191674\n",
      "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.009946\n",
      "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.012560\n",
      "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.008389\n",
      "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.013708\n",
      "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.029745\n",
      "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.033500\n",
      "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.018152\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.028340\n",
      "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.075638\n",
      "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.013629\n",
      "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.005156\n",
      "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.005084\n",
      "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.076775\n",
      "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.042420\n",
      "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.004184\n",
      "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.099585\n",
      "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.003081\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.013686\n",
      "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.007396\n",
      "\n",
      "Test set: Average loss: 0.0377, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "CPU times: user 1h 1min 17s, sys: 5min, total: 1h 6min 18s\n",
      "Wall time: 24min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Here you are, you have trained a model on remote data using Federated Learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
